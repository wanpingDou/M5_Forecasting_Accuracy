{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, sys, gc, warnings, psutil, random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3002725 entries, 0 to 3002724\n",
      "Data columns (total 34 columns):\n",
      " #   Column            Dtype   \n",
      "---  ------            -----   \n",
      " 0   id                category\n",
      " 1   item_id           category\n",
      " 2   dept_id           category\n",
      " 3   cat_id            category\n",
      " 4   store_id          category\n",
      " 5   state_id          category\n",
      " 6   d                 int16   \n",
      " 7   sales             float64 \n",
      " 8   release           int16   \n",
      " 9   sell_price        float16 \n",
      " 10  price_max         float16 \n",
      " 11  price_min         float16 \n",
      " 12  price_std         float16 \n",
      " 13  price_mean        float16 \n",
      " 14  price_norm        float16 \n",
      " 15  price_nunique     float16 \n",
      " 16  item_nunique      int16   \n",
      " 17  price_momentum    float16 \n",
      " 18  price_momentum_m  float16 \n",
      " 19  price_momentum_y  float16 \n",
      " 20  event_name_1      category\n",
      " 21  event_type_1      category\n",
      " 22  event_name_2      category\n",
      " 23  event_type_2      category\n",
      " 24  snap_CA           category\n",
      " 25  snap_TX           category\n",
      " 26  snap_WI           category\n",
      " 27  tm_d              int8    \n",
      " 28  tm_w              int8    \n",
      " 29  tm_m              int8    \n",
      " 30  tm_y              int8    \n",
      " 31  tm_wm             int8    \n",
      " 32  tm_dw             int8    \n",
      " 33  tm_w_end          int8    \n",
      "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 162.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_df = pd.concat([pd.read_pickle('../data/output/grid_part_1.pkl'),\n",
    "                     pd.read_pickle('../data/output/grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('../data/output/grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "\n",
    "\n",
    "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
    "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
    "\n",
    "# Let's \"inspect\" our grid DataFrame\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[402]\ttraining's rmse: 2.7867\tvalid_1's rmse: 2.39378\n"
     ]
    }
   ],
   "source": [
    "########################### Baseline model\n",
    "#################################################################################\n",
    "\n",
    "# We will need some global VARS for future\n",
    "\n",
    "SEED = 42  # Our random seed for everything\n",
    "random.seed(SEED)  # to make all tests \"deterministic\"\n",
    "np.random.seed(SEED)\n",
    "N_CORES = psutil.cpu_count()  # Available CPU cores\n",
    "\n",
    "TARGET = 'sales'  # Our Target\n",
    "END_TRAIN = 1941  # And we will use last 28 days as evaluation\n",
    "\n",
    "# Drop some items from \"TEST\" set part (1941...)\n",
    "grid_df = grid_df[grid_df['d'] <= END_TRAIN].reset_index(drop=True)\n",
    "\n",
    "# Features that we want to exclude from training\n",
    "remove_features = ['id', 'd', TARGET]\n",
    "\n",
    "# Our baseline model serves\n",
    "# to do fast checks of\n",
    "# new features performance\n",
    "\n",
    "# We will use LightGBM for our tests\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',  # Standart boosting type\n",
    "    'objective': 'regression',  # Standart loss for RMSE\n",
    "    'metric': ['rmse'],  # as we will use rmse as metric \"proxy\"\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'learning_rate': 0.05,  # 0.5 is \"fast enough\" for us\n",
    "    'num_leaves': 2**7 - 1,  # We will need model only for fast check\n",
    "    'min_data_in_leaf': 2**8 -\n",
    "    1,  # So we want it to train faster even with drop in generalization \n",
    "    'feature_fraction': 0.8,\n",
    "    'n_estimators':\n",
    "    5000,  # We don't want to limit training (you can change 5000 to any big enough number)\n",
    "    'early_stopping_rounds':\n",
    "    30,  # We will stop training almost immediately (if it stops improving) \n",
    "    'seed': SEED,\n",
    "    'verbose': -1,\n",
    "#     'device': 'gpu',\n",
    "#     'gpu_platform_id': -1,\n",
    "#     'gpu_device_id': -1\n",
    "}\n",
    "\n",
    "\n",
    "## RMSE\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
    "\n",
    "\n",
    "# Small function to make fast features tests\n",
    "# estimator = make_fast_test(grid_df)\n",
    "# it will return lgb booster for future analisys\n",
    "def make_fast_test(df):\n",
    "\n",
    "    features_columns = [col for col in list(df) if col not in remove_features]\n",
    "\n",
    "    tr_x, tr_y = df[df['d'] <= (END_TRAIN - 28)][features_columns], df[\n",
    "        df['d'] <= (END_TRAIN - 28)][TARGET]\n",
    "    vl_x, v_y = df[df['d'] > (END_TRAIN - 28)][features_columns], df[\n",
    "        df['d'] > (END_TRAIN - 28)][TARGET]\n",
    "\n",
    "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
    "\n",
    "    estimator = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, valid_data],\n",
    "        verbose_eval=500,\n",
    "    )\n",
    "\n",
    "    return estimator\n",
    "\n",
    "\n",
    "# Make baseline model\n",
    "baseline_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[290]\ttraining's rmse: 2.57757\tvalid_1's rmse: 2.26148\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "########################### Some more info about lags here:\n",
    "\n",
    "\n",
    "# Small helper to make lags creation faster\n",
    "from multiprocessing import Pool                # Multiprocess Runs\n",
    "\n",
    "## Multiprocessing Run.\n",
    "# :t_split - int of lags days                   # type: int\n",
    "# :func - Function to apply on each split       # type: python function\n",
    "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def make_normal_lag(lag_day):\n",
    "    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n",
    "    col_name = 'sales_lag_'+str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "# Launch parallel lag creation\n",
    "# and \"append\" to our grid\n",
    "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart RMSE 2.2614800841221525\n",
      "release 0.0\n",
      "sell_price 0.0044\n",
      "price_max 0.0012\n",
      "price_min 0.0002\n",
      "price_std 0.0019\n",
      "price_mean 0.0024\n",
      "price_norm 0.0085\n",
      "price_nunique -0.0001\n",
      "item_nunique 0.0006\n",
      "price_momentum -0.0001\n",
      "price_momentum_m 0.0053\n",
      "price_momentum_y 0.0004\n",
      "tm_d 0.0084\n",
      "tm_w -0.0002\n",
      "tm_m -0.0001\n",
      "tm_y 0.0\n",
      "tm_wm 0.0005\n",
      "tm_dw 0.139\n",
      "tm_w_end 0.0111\n",
      "sales_lag_1 0.6005\n",
      "sales_lag_2 0.045\n",
      "sales_lag_3 0.0221\n",
      "sales_lag_4 0.0184\n",
      "sales_lag_5 0.0196\n",
      "sales_lag_6 0.0192\n",
      "sales_lag_7 0.0405\n"
     ]
    }
   ],
   "source": [
    "########################### Permutation importance Test\n",
    "\n",
    "# Let's creat evaluation dataset and features\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "evaluation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "\n",
    "# Make normal prediction with our model and save score\n",
    "evaluation_df['preds'] = test_model.predict(evaluation_df[features_columns])\n",
    "base_score = rmse(evaluation_df[TARGET], evaluation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "\n",
    "# Now we are looping over all our numerical features\n",
    "for col in features_columns:\n",
    "    \n",
    "    # We will make evaluation set copy to restore\n",
    "    # features states on each run\n",
    "    temp_df = evaluation_df.copy()\n",
    "    \n",
    "    # Error here appears if we have \"categorical\" features and can't \n",
    "    # do np.random.permutation without disrupt categories\n",
    "    # so we need to check if feature is numerical\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        \n",
    "        # If our current rmse score is less than base score\n",
    "        # it means that feature most probably is a bad one\n",
    "        # and our model is learning on noise\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "# Remove Temp data\n",
    "del temp_df, evaluation_df\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from eli5 documentation (seems it's perfect explanation)\n",
    "\n",
    "The idea is the following: feature importance can be measured by looking at how much the score (accuracy, mse, rmse, mae, etc. - any score we’re interested in) decreases when a feature is not available.\n",
    "\n",
    "To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n",
    "\n",
    "To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can **replace it with random noise** - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the **same distribution as original feature values** (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
    "\n",
    "---\n",
    "\n",
    "It's not good when feature remove (replaced by noise) but we have better score. Simple and easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttraining's rmse: 2.86407\tvalid_1's rmse: 2.41443\n",
      "Standart RMSE 2.4144328608930605\n",
      "release 0.0\n",
      "sell_price 0.0163\n",
      "price_max 0.0048\n",
      "price_min 0.0041\n",
      "price_std 0.0064\n",
      "price_mean 0.004\n",
      "price_norm 0.0518\n",
      "price_nunique 0.0226\n",
      "item_nunique 0.0087\n",
      "price_momentum 0.0\n",
      "price_momentum_m 0.0281\n",
      "price_momentum_y 0.0133\n",
      "tm_d 0.0048\n",
      "tm_w 0.0009\n",
      "tm_m -0.0001\n",
      "tm_y 0.0\n",
      "tm_wm -0.0\n",
      "tm_dw 0.1204\n",
      "tm_w_end 0.0101\n",
      "sales_lag_56 0.0254\n",
      "sales_lag_57 0.0092\n",
      "sales_lag_58 -0.0005\n",
      "sales_lag_59 0.0016\n",
      "sales_lag_60 -0.0016\n",
      "sales_lag_61 0.0014\n",
      "sales_lag_62 0.0089\n"
     ]
    }
   ],
   "source": [
    "########################### Lets test far away Lags (7 days with 56 days shift)\n",
    "########################### and check permutation importance\n",
    "#################################################################################\n",
    "\n",
    "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "evaluation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "evaluation_df['preds'] = test_model.predict(evaluation_df[features_columns])\n",
    "base_score = rmse(evaluation_df[TARGET], evaluation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "for col in features_columns:\n",
    "    temp_df = evaluation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "del temp_df, evaluation_df\n",
    "        \n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# Results:\n",
    "## Lags with 56 days shift (far away past) are not as important\n",
    "## as nearest past lags\n",
    "## and at some point will be just noise for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: id 7\n",
      "[0.72224136 0.06621842 0.05938444 0.04201445 0.03891686 0.03614344\n",
      " 0.03508102]\n",
      "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's rmse: 2.64819\tvalid_1's rmse: 2.26544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def make_pca(df, pca_col, n_days):\n",
    "    print('PCA:', pca_col, n_days)\n",
    "    \n",
    "    # We don't need any other columns to make pca\n",
    "    pca_df = df[[pca_col,'d',TARGET]]\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\" \n",
    "    # we need to agg first\n",
    "    if pca_col != 'id':\n",
    "        merge_base = pca_df[[pca_col,'d']]\n",
    "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
    "        pca_df[TARGET] = pca_df['sum']\n",
    "        del pca_df['sum']\n",
    "    \n",
    "    # Min/Max scaling\n",
    "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
    "    \n",
    "    # Making \"lag\" in old way (not parallel)\n",
    "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
    "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
    "    pca_df = pca_df.assign(**{\n",
    "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
    "            for l in LAG_DAYS\n",
    "            for col in [TARGET]\n",
    "        })\n",
    "    \n",
    "    pca_columns = list(pca_df)[3:]\n",
    "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
    "    pca = PCA(random_state=SEED)\n",
    "    \n",
    "    # You can use fit_transform here\n",
    "    pca.fit(pca_df[pca_columns])\n",
    "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
    "    \n",
    "    print(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
    "    keep_cols = pca_columns[:3]\n",
    "    print('Columns to keep:', keep_cols)\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\"\n",
    "    # we need merge back our results to merge_base df\n",
    "    # and only than return resulted df\n",
    "    # I'll skip that step here\n",
    "    \n",
    "    return pca_df[keep_cols]\n",
    "\n",
    "\n",
    "# Make PCA\n",
    "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id\n",
      "Encoding cat_id\n",
      "Encoding dept_id\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttraining's rmse: 2.80608\tvalid_1's rmse: 2.39575\n"
     ]
    }
   ],
   "source": [
    "########################### Mean/std target encoding\n",
    "#################################################################################\n",
    "\n",
    "# We will use these three columns for test\n",
    "# (in combination with store_id)\n",
    "icols = ['item_id','cat_id','dept_id']\n",
    "\n",
    "# But we can use any other column or even multiple groups\n",
    "# like these ones\n",
    "#            'state_id',\n",
    "#            'store_id',\n",
    "#            'cat_id',\n",
    "#            'dept_id',\n",
    "#            ['state_id', 'cat_id'],\n",
    "#            ['state_id', 'dept_id'],\n",
    "#            ['store_id', 'cat_id'],\n",
    "#            ['store_id', 'dept_id'],\n",
    "#            'item_id',\n",
    "#            ['item_id', 'state_id'],\n",
    "#            ['item_id', 'store_id']\n",
    "\n",
    "# There are several ways to do \"mean\" encoding\n",
    "## K-fold scheme\n",
    "## LOO (leave one out)\n",
    "## Smoothed/regularized \n",
    "## Expanding mean\n",
    "## etc \n",
    "\n",
    "# You can test as many options as you want\n",
    "# and decide what to use\n",
    "# Because of memory issues you can't \n",
    "# use many features.\n",
    "\n",
    "# We will use simple target encoding\n",
    "# by std and mean agg\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    temp_df = grid_df[grid_df['d']<=(1941-28)] # to be sure we don't have leakage in our evaluation set\n",
    "    \n",
    "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
    "    joiner = '_'+col+'_encoding_'\n",
    "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
    "    temp_df = temp_df.reset_index()\n",
    "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
    "    del temp_df\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "# Bad thing that for some items  \n",
    "# we are using past and future values.\n",
    "# But we are looking for \"categorical\" similiarity\n",
    "# on a \"long run\". So future here is not a big problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's rmse: 2.63046\tvalid_1's rmse: 2.29666\n",
      "Early stopping, best iteration is:\n",
      "[494]\ttraining's rmse: 2.63187\tvalid_1's rmse: 2.2964\n"
     ]
    }
   ],
   "source": [
    "########################### Last non O sale\n",
    "#################################################################################\n",
    "\n",
    "def find_last_sale(df,n_day):\n",
    "    \n",
    "    # Limit initial df\n",
    "    ls_df = df[['id','d',TARGET]]\n",
    "    \n",
    "    # Convert target to binary\n",
    "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
    "    \n",
    "    # Make lags to prevent any leakage\n",
    "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
    "\n",
    "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
    "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
    "\n",
    "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
    "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
    "\n",
    "    return ls_df[['last_sale']]\n",
    "\n",
    "\n",
    "# Find last non zero\n",
    "# Need some \"dances\" to fit in memory limit with groupers\n",
    "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['state_id']\n",
      "Encoding ['store_id']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['dept_id']\n",
      "Encoding ['state_id', 'cat_id']\n",
      "Encoding ['state_id', 'dept_id']\n",
      "Encoding ['store_id', 'cat_id']\n",
      "Encoding ['store_id', 'dept_id']\n",
      "Encoding ['item_id']\n",
      "Encoding ['item_id', 'state_id']\n",
      "Encoding ['item_id', 'store_id']\n"
     ]
    }
   ],
   "source": [
    "########################### Apply on grid_df\n",
    "#################################################################################\n",
    "# lets read grid from \n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "# to be sure that our grids are aligned by index\n",
    "grid_df = pd.read_pickle('../data/output/grid_part_1.pkl')\n",
    "grid_df[TARGET][grid_df['d']>(1941-28)] = np.nan\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "            ]\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
    "\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['id','d']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('../data/output/mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 24 columns):\n",
      " #   Column                     Dtype   \n",
      "---  ------                     -----   \n",
      " 0   id                         category\n",
      " 1   d                          int16   \n",
      " 2   enc_state_id_mean          float16 \n",
      " 3   enc_state_id_std           float16 \n",
      " 4   enc_store_id_mean          float16 \n",
      " 5   enc_store_id_std           float16 \n",
      " 6   enc_cat_id_mean            float16 \n",
      " 7   enc_cat_id_std             float16 \n",
      " 8   enc_dept_id_mean           float16 \n",
      " 9   enc_dept_id_std            float16 \n",
      " 10  enc_state_id_cat_id_mean   float16 \n",
      " 11  enc_state_id_cat_id_std    float16 \n",
      " 12  enc_state_id_dept_id_mean  float16 \n",
      " 13  enc_state_id_dept_id_std   float16 \n",
      " 14  enc_store_id_cat_id_mean   float16 \n",
      " 15  enc_store_id_cat_id_std    float16 \n",
      " 16  enc_store_id_dept_id_mean  float16 \n",
      " 17  enc_store_id_dept_id_std   float16 \n",
      " 18  enc_item_id_mean           float16 \n",
      " 19  enc_item_id_std            float16 \n",
      " 20  enc_item_id_state_id_mean  float16 \n",
      " 21  enc_item_id_state_id_std   float16 \n",
      " 22  enc_item_id_store_id_mean  float16 \n",
      " 23  enc_item_id_store_id_std   float16 \n",
      "dtypes: category(1), float16(22), int16(1)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of new features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>enc_state_id_mean</th>\n",
       "      <th>enc_state_id_std</th>\n",
       "      <th>enc_store_id_mean</th>\n",
       "      <th>enc_store_id_std</th>\n",
       "      <th>enc_cat_id_mean</th>\n",
       "      <th>enc_cat_id_std</th>\n",
       "      <th>enc_dept_id_mean</th>\n",
       "      <th>enc_dept_id_std</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_store_id_cat_id_mean</th>\n",
       "      <th>enc_store_id_cat_id_std</th>\n",
       "      <th>enc_store_id_dept_id_mean</th>\n",
       "      <th>enc_store_id_dept_id_std</th>\n",
       "      <th>enc_item_id_mean</th>\n",
       "      <th>enc_item_id_std</th>\n",
       "      <th>enc_item_id_state_id_mean</th>\n",
       "      <th>enc_item_id_state_id_std</th>\n",
       "      <th>enc_item_id_store_id_mean</th>\n",
       "      <th>enc_item_id_store_id_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47735392</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>3.939453</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>4.496094</td>\n",
       "      <td>2.107422</td>\n",
       "      <td>5.753906</td>\n",
       "      <td>2.623047</td>\n",
       "      <td>7.03125</td>\n",
       "      <td>...</td>\n",
       "      <td>2.138672</td>\n",
       "      <td>6.148438</td>\n",
       "      <td>2.830078</td>\n",
       "      <td>7.707031</td>\n",
       "      <td>0.841797</td>\n",
       "      <td>1.744141</td>\n",
       "      <td>0.512695</td>\n",
       "      <td>1.068359</td>\n",
       "      <td>0.534180</td>\n",
       "      <td>1.177734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735393</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>3.939453</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>4.496094</td>\n",
       "      <td>2.107422</td>\n",
       "      <td>5.753906</td>\n",
       "      <td>2.623047</td>\n",
       "      <td>7.03125</td>\n",
       "      <td>...</td>\n",
       "      <td>2.138672</td>\n",
       "      <td>6.148438</td>\n",
       "      <td>2.830078</td>\n",
       "      <td>7.707031</td>\n",
       "      <td>0.435059</td>\n",
       "      <td>0.947266</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.782715</td>\n",
       "      <td>0.376465</td>\n",
       "      <td>0.819824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735394</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>3.939453</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>4.496094</td>\n",
       "      <td>2.107422</td>\n",
       "      <td>5.753906</td>\n",
       "      <td>2.623047</td>\n",
       "      <td>7.03125</td>\n",
       "      <td>...</td>\n",
       "      <td>2.138672</td>\n",
       "      <td>6.148438</td>\n",
       "      <td>2.830078</td>\n",
       "      <td>7.707031</td>\n",
       "      <td>0.708008</td>\n",
       "      <td>1.202148</td>\n",
       "      <td>0.625977</td>\n",
       "      <td>1.140625</td>\n",
       "      <td>0.895020</td>\n",
       "      <td>1.385742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735395</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>3.939453</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>4.496094</td>\n",
       "      <td>2.107422</td>\n",
       "      <td>5.753906</td>\n",
       "      <td>2.623047</td>\n",
       "      <td>7.03125</td>\n",
       "      <td>...</td>\n",
       "      <td>2.138672</td>\n",
       "      <td>6.148438</td>\n",
       "      <td>2.830078</td>\n",
       "      <td>7.707031</td>\n",
       "      <td>1.113281</td>\n",
       "      <td>1.479492</td>\n",
       "      <td>1.036133</td>\n",
       "      <td>1.580078</td>\n",
       "      <td>0.720215</td>\n",
       "      <td>1.249023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735396</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>1.320312</td>\n",
       "      <td>3.939453</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>4.496094</td>\n",
       "      <td>2.107422</td>\n",
       "      <td>5.753906</td>\n",
       "      <td>2.623047</td>\n",
       "      <td>7.03125</td>\n",
       "      <td>...</td>\n",
       "      <td>2.138672</td>\n",
       "      <td>6.148438</td>\n",
       "      <td>2.830078</td>\n",
       "      <td>7.707031</td>\n",
       "      <td>1.684570</td>\n",
       "      <td>3.134766</td>\n",
       "      <td>2.332031</td>\n",
       "      <td>2.947266</td>\n",
       "      <td>1.690430</td>\n",
       "      <td>1.964844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id     d  enc_state_id_mean  \\\n",
       "47735392  FOODS_3_823_WI_3_evaluation  1969           1.320312   \n",
       "47735393  FOODS_3_824_WI_3_evaluation  1969           1.320312   \n",
       "47735394  FOODS_3_825_WI_3_evaluation  1969           1.320312   \n",
       "47735395  FOODS_3_826_WI_3_evaluation  1969           1.320312   \n",
       "47735396  FOODS_3_827_WI_3_evaluation  1969           1.320312   \n",
       "\n",
       "          enc_state_id_std  enc_store_id_mean  enc_store_id_std  \\\n",
       "47735392          3.939453           1.371094          4.496094   \n",
       "47735393          3.939453           1.371094          4.496094   \n",
       "47735394          3.939453           1.371094          4.496094   \n",
       "47735395          3.939453           1.371094          4.496094   \n",
       "47735396          3.939453           1.371094          4.496094   \n",
       "\n",
       "          enc_cat_id_mean  enc_cat_id_std  enc_dept_id_mean  enc_dept_id_std  \\\n",
       "47735392         2.107422        5.753906          2.623047          7.03125   \n",
       "47735393         2.107422        5.753906          2.623047          7.03125   \n",
       "47735394         2.107422        5.753906          2.623047          7.03125   \n",
       "47735395         2.107422        5.753906          2.623047          7.03125   \n",
       "47735396         2.107422        5.753906          2.623047          7.03125   \n",
       "\n",
       "          ...  enc_store_id_cat_id_mean  enc_store_id_cat_id_std  \\\n",
       "47735392  ...                  2.138672                 6.148438   \n",
       "47735393  ...                  2.138672                 6.148438   \n",
       "47735394  ...                  2.138672                 6.148438   \n",
       "47735395  ...                  2.138672                 6.148438   \n",
       "47735396  ...                  2.138672                 6.148438   \n",
       "\n",
       "          enc_store_id_dept_id_mean  enc_store_id_dept_id_std  \\\n",
       "47735392                   2.830078                  7.707031   \n",
       "47735393                   2.830078                  7.707031   \n",
       "47735394                   2.830078                  7.707031   \n",
       "47735395                   2.830078                  7.707031   \n",
       "47735396                   2.830078                  7.707031   \n",
       "\n",
       "          enc_item_id_mean  enc_item_id_std  enc_item_id_state_id_mean  \\\n",
       "47735392          0.841797         1.744141                   0.512695   \n",
       "47735393          0.435059         0.947266                   0.366943   \n",
       "47735394          0.708008         1.202148                   0.625977   \n",
       "47735395          1.113281         1.479492                   1.036133   \n",
       "47735396          1.684570         3.134766                   2.332031   \n",
       "\n",
       "          enc_item_id_state_id_std  enc_item_id_store_id_mean  \\\n",
       "47735392                  1.068359                   0.534180   \n",
       "47735393                  0.782715                   0.376465   \n",
       "47735394                  1.140625                   0.895020   \n",
       "47735395                  1.580078                   0.720215   \n",
       "47735396                  2.947266                   1.690430   \n",
       "\n",
       "          enc_item_id_store_id_std  \n",
       "47735392                  1.177734  \n",
       "47735393                  0.819824  \n",
       "47735394                  1.385742  \n",
       "47735395                  1.249023  \n",
       "47735396                  1.964844  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
